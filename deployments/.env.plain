APP_PORT=8000
LLM_RUNTIME=llama_cpp

# LlamaModel
LLAMA_MODEL_PATH=./local/Meta-Llama-3.1-8B-Instruct-Q4_K_M-take2.gguf
LLAMA_MODEL_N_CTX=8192
LLAMA_MODEL_TOP_K=30
LLAMA_MODEL_TOP_P=0.9
LLAMA_MODEL_TEMPERATURE=0.6
LLAMA_MODEL_REPEAT_PENALTY=1.1
LLAMA_MODEL_VERBOSE=False
LLAMA_MODEL_N_GPU_LAYERS=-1

# Local
BENCH_LALA_ENDPOINT=http://localhost:8000/chat
BENCH_LALA_CONCURRENT_REQUESTS=10
BENCH_LALA_TOTAL_REQUESTS=20